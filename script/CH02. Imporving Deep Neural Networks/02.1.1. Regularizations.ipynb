{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH02.1. **Regularizations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **L2(Ridge) Regularization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **[GRAPH]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 비용 함수에 가중치 프로베니우스 놈(Frobenius norm)을 추가하는 방법\n",
    "#### ※ norm : 벡터의 크기나 길이를 측정하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 직관적 이해** : **[CONTENTS]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 적용 방식** :\n",
    "#### $ \\hspace{0.15cm} $ ① Cost function\n",
    "#### $ \\hspace{0.6cm} J(W^{[1]},\\, \\textbf{b}^{[1]},\\, \\cdots{},\\, W^{[L]},\\, \\textbf{b}^{[L]}) = \\frac{1}{m} \\cdot{} \\displaystyle\\sum^{m}_{i=1} \\ell(a^{[L](i)},\\, y^{(i)}) + \\frac{\\lambda{}}{2m} \\cdot{} \\overbrace{\\displaystyle\\sum^{L}_{l=1} || W^{[l]} ||^{2}_{F}}^{\\text{Frobenius norm}} $\n",
    "#### $ \\hspace{4.95cm} = \\frac{1}{m} \\cdot{} \\displaystyle\\sum^{m}_{i=1} \\ell(a^{[L](i)},\\, y^{(i)}) + \\frac{\\lambda{}}{2m} \\cdot{} \\displaystyle\\sum^{L}_{l=1} \\displaystyle\\sum^{i=n^{[l]}}_{i=1} \\displaystyle\\sum^{n^{[l-1]}}_{k=1} (w^{[l]}_{i,\\,k})^{2} $\n",
    "#### $ \\hspace{0.15cm} $ ② Back-propagation\n",
    "#### $ \\hspace{0.6cm} \\frac{\\partial{}J}{\\partial{}W^{[l]}} = \\frac{1}{m} \\cdot{} \\frac{\\partial{}J}{\\partial{}Z^{[l]}} \\cdot{} (A^{[l-2]})^{T} + \\frac{\\lambda{}}{m} \\cdot{} W^{[l]} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 기능** : 가중치 크기 제한(weight decay) $ \\rightarrow{} $ 모델 복잡성 감소 $ \\rightarrow{} $ 과적합(overfitting) 방지 $ \\rightarrow{} $ 일반화 성능 향상"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Drop Out**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **[GRAPH]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 모델이 **학습할 때** 은닉층(hidden layer)의 각 노드에 대해 활성화/비활성화할지를 결정하는 베르누이 시행을 적용하는 방법 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 직관적 이해** : **[CONTENTS]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 적용 방식** :\n",
    "#### $ \\hspace{0.15cm} $ ① Activate Function\n",
    "#### $ \\hspace{0.6cm} \\underset{n^{[l]}\\times{}m}{D^{[l]}} = \\begin{bmatrix} d^{[l](1)}_{1} & d^{[l](2)}_{1} & \\cdots{} & d^{[l](m)}_{1} \\\\ d^{[l](1)}_{2} & d^{[l](2)}_{2} & \\cdots{} & d^{[l](m)}_{2} \\\\ \\vdots{} & \\vdots{} & \\ddots{} & \\vdots{} \\\\ d^{[l](1)}_{n^{[l]}} & d^{[l](2)}_{n^{[l]}} & \\cdots{} & d^{[l](m)}_{n^{[l]}} \\end{bmatrix} \\;\\; $ where $ \\, d_{i,j} \\, \\sim{} \\text{Bernoulli}(p=\\text{keep-prop}) $\n",
    "#### $ \\hspace{0.6cm} \\underset{n^{[l]}\\times{}m}{A^{[l]}} = \\frac{1}{p} \\cdot{} D^{[l]} \\odot{} g(Z^{[l]}) $\n",
    "#### $ \\hspace{0.15cm} $ ② Back-propagation\n",
    "#### $ \\hspace{0.6cm} \\frac{\\partial{}J}{\\partial{}A^{[k]}} = \\frac{1}{p} \\cdot{} D^{[l]} \\odot{} (W^{[k+1]})^{T} \\cdot{} \\frac{\\partial{}J}{\\partial{}Z^{[k+1]}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(`WHY?`) $ \\, \\frac{1}{p} $ 를 곱하는 이유** :\n",
    "#### $ \\hspace{0.15cm} p $ 비율만큼 출력($ A^{[l]} $) 기대값이 작아지고 역전파 때 그라디언트가 기존보다 작게 추정되어, 학습이 불안정해지기 때문에 보정 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 기능** : 특정 노드의 가중치 의존 제한(가중치 분산) $ \\, \\rightarrow{} $ 앙상블 효과 $ \\rightarrow{} $ 과적합 방지 $ \\rightarrow{} $ 일반화 성능 향상\n",
    "#### ※ 앙상블 효과 : 무작위로 선택된 노드의 부분 집합만이 활성화되면서, 여러 다른 모델처럼 작동하는 효과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Early Stopping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **[GRAPH]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 모델의 훈련 성능이 더 이상 개선되지 않을 때 학습을 조기 종료(중단)하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 직관적 이해** : **[CONTENTS]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 적용 방식** : **[CONTENTS]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 기능** : **[CONTENTS]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(PLUS)** Early Stopping은 비용 최적화와 정규화를 동시에 수행하기 때문에, 이를 독립적으로 처리하기는 힘듬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
